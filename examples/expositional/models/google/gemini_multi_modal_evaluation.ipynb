{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ut97AtzeF6N_",
   "metadata": {},
   "source": [
    "# Multimodal Evaluations with Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Par9tN3ZPQxy",
   "metadata": {},
   "source": [
    "Installing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5nCrN9PepqQg",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trulens google-genai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64168892",
   "metadata": {},
   "source": [
    "## Download data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UIke4iCTpkL4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://docs.google.com/uc?export=download&id=1ShPnYVc1iL_TA1t7ErCFEAHT74-qvMrn\" -O ./sf.png\n",
    "!wget \"https://docs.google.com/uc?export=download&id=16oTISaB5m2uasHlezg7iPYV2FBiQYc4n\" -O ./customer_support_agnet.wav\n",
    "!wget \"https://docs.google.com/uc?export=download&id=1186BiByf2NUXmOOO8k7hGK2qGy8o5fCb\" -O ./chameleon.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf57a68",
   "metadata": {},
   "source": [
    "## Setting Gemini Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n",
    "google_client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2964e",
   "metadata": {},
   "source": [
    "## Setup custom provider with Gemini\n",
    "\n",
    "In this tutorial, we leverage the multi-modal capabilities of Gemini models to evaluate across different modalities, while using their structured output generation to reliably produce scores in the desired result format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ba65d",
   "metadata": {},
   "source": [
    "### For images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vfDPiqlGJEll",
   "metadata": {},
   "source": [
    "Gemini models supports [image formats](https://ai.google.dev/gemini-api/docs/image-understanding#supported-formats) (JPEG, PNG, WebP, HEIC, HEIF), the `get_image_mime_type` function ensures we correctly identify and pass the image in a supported type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YQAO0h_jJGIQ",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_mime_type(file_bytes: bytes) -> str:\n",
    "    \"\"\"\n",
    "    Detect the image type based only on file bytes and return (bytes, mime_type).\n",
    "\n",
    "    Supported formats:\n",
    "        - JPEG (.jpg, .jpeg) → image/jpeg\n",
    "        - PNG (.png)         → image/png\n",
    "        - WEBP (.webp)       → image/webp\n",
    "        - HEIC (.heic)       → image/heic\n",
    "        - HEIF (.heif)       → image/heif\n",
    "\n",
    "    Args:\n",
    "        file_bytes (bytes): Raw file content.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[bytes, str]: (original bytes, mime type string)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file type is unsupported or cannot be detected.\n",
    "    \"\"\"\n",
    "    if file_bytes.startswith(b\"\\xff\\xd8\\xff\"):\n",
    "        return \"image/jpeg\"\n",
    "\n",
    "    if file_bytes.startswith(b\"\\x89PNG\\r\\n\\x1a\\n\"):\n",
    "        return \"image/png\"\n",
    "\n",
    "    if file_bytes[0:4] == b\"RIFF\" and file_bytes[8:12] == b\"WEBP\":\n",
    "        return \"image/webp\"\n",
    "\n",
    "    if file_bytes[4:8] == b\"ftyp\":\n",
    "        brand = file_bytes[8:12]\n",
    "        if brand in [b\"heic\", b\"heix\", b\"hevc\", b\"hevx\"]:\n",
    "            return \"image/heic\"\n",
    "        if brand in [b\"mif1\", b\"msf1\", b\"heif\"]:\n",
    "            return \"image/heif\"\n",
    "\n",
    "    raise ValueError(\"Unsupported or unrecognized image format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fyTmn1yuKSeA",
   "metadata": {},
   "source": [
    "#### Gemini Feedback Provider for evaluating Image Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cacfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.feedback import Provider\n",
    "from pydantic import BaseModel, Field\n",
    "from google.genai import types\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ImageFaithfulnessScore(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a binary faithfulness score for an image response\n",
    "    with respect to the given query and/or retrieved context.\n",
    "    \"\"\"\n",
    "\n",
    "    value: float = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Binary faithfulness score. \"\n",
    "            \"1.0 → The image is faithful (accurately reflects the query/context). \"\n",
    "            \"0.0 → The image is unfaithful (introduces unsupported or contradictory content).\"\n",
    "        ),\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "    )\n",
    "\n",
    "    reason: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A concise explanation describing why this score was given. \"\n",
    "            \"Should reference objects, attributes, or details in the image \"\n",
    "            \"and whether they are supported by the query/context.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class Gemini_Provider(Provider):\n",
    "    def multi_modal_faithfulness(\n",
    "        self, query: str, retrieved_context: List\n",
    "    ):\n",
    "        retrieved_context = [\n",
    "            (\n",
    "                types.Part(text=rc)\n",
    "                if isinstance(rc, str)\n",
    "                else types.Part.from_bytes(data=rc, mime_type=get_image_mime_type(rc))\n",
    "            )\n",
    "            for rc in retrieved_context\n",
    "        ]\n",
    "        score = google_client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "You are an AI system designed to judge whether a given piece of information is supported by the provided context, which may include both textual and visual content.\n",
    "\n",
    "### TASK:\n",
    "\n",
    "Analyze the provided **information statement** and the **context** (including text and any images if available).\n",
    "Determine whether the information is supported by the context.\n",
    "\n",
    "Consider these factors:\n",
    "- **Support from Text**: Does the textual context explicitly or implicitly support the information?\n",
    "- **Support from Visuals**: If images are provided, do they support the information?\n",
    "- **Partial Evidence**: If any part of the context (text or image) supports the information, output **1**.\n",
    "- **Contradiction or Absence**: If the context does not support or contradicts the information, output **0**.\n",
    "\n",
    "The classification must be one of the following:\n",
    "[1, 0]\n",
    "\n",
    "IMPORTANT:\n",
    "- \"1\" → At least one piece of context (text or image) supports the information.\n",
    "- \"0\" → None of the context supports the information, or it contradicts it.\n",
    "\n",
    "************\n",
    "\n",
    "Here is the information statement:\n",
    "\"\"\"\n",
    "                ),\n",
    "                types.Part(text=query),\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "\n",
    "Here is the context:\n",
    "\"\"\"\n",
    "                ),\n",
    "                *retrieved_context,\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "\n",
    "************\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Provide a single digit (`1` or `0`) representing the judgment.\n",
    "\n",
    "************\n",
    "\n",
    "### EXAMPLES:\n",
    "\n",
    "Information: Apple pie is generally double-crusted.\n",
    "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
    "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "Answer: 1\n",
    "\n",
    "Information: Apple pies taste bad.\n",
    "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
    "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "Answer: 0\n",
    "\n",
    "************\n",
    "\n",
    "Analyze the information statement and the context, and respond in this format.\n",
    "\"\"\"\n",
    "                ),\n",
    "            ],\n",
    "            config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": ImageFaithfulnessScore,\n",
    "            },\n",
    "        )\n",
    "        return score.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3lT4L120HWts",
   "metadata": {},
   "source": [
    "#### Test custom feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_provider = Gemini_Provider()\n",
    "\n",
    "image_file_name = \"sf.png\"\n",
    "with open(image_file_name, \"rb\") as f:\n",
    "    image_bytes = f.read()\n",
    "\n",
    "faithfulness = gemini_provider.multi_modal_faithfulness(\n",
    "    query=\"Does Sam’s Grill have outdoor seating?\",\n",
    "    retrieved_context=[\n",
    "        image_bytes,\n",
    "        \"Customers can choose dine-in, curbside pickup, or delivery.\",\n",
    "    ],\n",
    ")\n",
    "faithfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babc535",
   "metadata": {},
   "source": [
    "### For Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dG1ImPHTJ5-l",
   "metadata": {},
   "source": [
    "Similarly, for audio we validate the file type with `get_audio_mime_type`, since Gemini supports certain [audio formats](https://ai.google.dev/gemini-api/docs/audio#supported-formats) (WAV, MP3, AIFF, AAC, OGG, FLAC), and passing them correctly requires this conversion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eSM9Q1PCJ5w9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_mime_type(file_bytes: bytes) -> str:\n",
    "    \"\"\"\n",
    "    Detect the audio type based only on file bytes and return the MIME type.\n",
    "\n",
    "    Supported formats:\n",
    "        - WAV (.wav)          → audio/wav\n",
    "        - MP3 (.mp3)          → audio/mp3\n",
    "        - AIFF (.aiff, .aif)  → audio/aiff\n",
    "        - AAC (.aac)          → audio/aac\n",
    "        - OGG Vorbis (.ogg)   → audio/ogg\n",
    "        - FLAC (.flac)        → audio/flac\n",
    "\n",
    "    Args:\n",
    "        file_bytes (bytes): Raw file content.\n",
    "\n",
    "    Returns:\n",
    "        str: MIME type string.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file type is unsupported or cannot be detected.\n",
    "    \"\"\"\n",
    "    if file_bytes[0:4] == b\"RIFF\" and file_bytes[8:12] == b\"WAVE\":\n",
    "        return \"audio/wav\"\n",
    "\n",
    "    if file_bytes[0:3] == b\"ID3\" or (file_bytes[0] == 0xFF and (file_bytes[1] & 0xE0) == 0xE0):\n",
    "        return \"audio/mp3\"\n",
    "\n",
    "    if file_bytes[0:4] == b\"FORM\" and file_bytes[8:12] == b\"AIFF\":\n",
    "        return \"audio/aiff\"\n",
    "\n",
    "    if len(file_bytes) > 2 and file_bytes[0] == 0xFF and (file_bytes[1] & 0xF0) == 0xF0:\n",
    "        return \"audio/aac\"\n",
    "\n",
    "    if file_bytes[0:4] == b\"OggS\":\n",
    "        return \"audio/ogg\"\n",
    "\n",
    "    if file_bytes[0:4] == b\"fLaC\":\n",
    "        return \"audio/flac\"\n",
    "\n",
    "    raise ValueError(\"Unsupported or unrecognized audio format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5RyPh8MLN_g",
   "metadata": {},
   "source": [
    "#### Evaluating Customer Support Chatbot Resolutions with Gemini Feedback Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.feedback import Provider\n",
    "from pydantic import BaseModel, Field\n",
    "from google.genai import types\n",
    "\n",
    "class ResolutionStatus(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents whether the support issue was resolved based on the agent's final utterance.\n",
    "    \"\"\"\n",
    "    resolved: bool = Field(\n",
    "        ...,\n",
    "        description=\"True if the final utterance clearly indicates resolution of the issue; False otherwise.\"\n",
    "    )\n",
    "\n",
    "    reason: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A short explanation referencing the agent's final words \"\n",
    "            \"and the detected emotion (tone, confidence, reassurance).\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class Gemini_Provider(Provider):\n",
    "    def audio_resolution_detection(self, audio_bytes: bytes):\n",
    "        result = google_client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[\n",
    "                \"\"\"You are an AI system that checks customer support call endings.\n",
    "\n",
    "### TASK:\n",
    "\n",
    "1. Compare the provided transcript with the audio and verify *faithfulness* (did the audio actually say the transcript?).\n",
    "2. Based on both the transcript meaning AND the detected emotion in the audio, determine if the issue was **resolved**.\n",
    "\n",
    "Guidelines for resolution:\n",
    "- If the final utterance provides a clear action, resolution, or timeline in a confident or neutral/reassuring tone → resolved = True.\n",
    "- If the final utterance is vague, evasive, non-committal, or delivered with frustration/hesitation → resolved = False.\n",
    "\n",
    "************\n",
    "\n",
    "Here is the audio to analyze:\n",
    "\"\"\",\n",
    "                types.Part.from_bytes(\n",
    "                    data=audio_bytes,\n",
    "                    mime_type=get_audio_mime_type(audio_bytes),\n",
    "                ),\n",
    "                \"\"\"\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Return JSON in the following schema:\n",
    "{\n",
    "  \"resolved\": true/false,\n",
    "  \"reason\": \"short explanation with reference to transcript + audio tone\"\n",
    "}\n",
    "\"\"\",\n",
    "            ],\n",
    "            config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": ResolutionStatus,\n",
    "            },\n",
    "        )\n",
    "        return result.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G8D-M52OHd78",
   "metadata": {},
   "source": [
    "#### Test custom feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de530cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_provider = Gemini_Provider()\n",
    "\n",
    "# Only for audio of size <20Mb\n",
    "with open(\"customer_support_agnet.wav\", \"rb\") as f:\n",
    "    audio_bytes = f.read()\n",
    "\n",
    "gemini_provider.audio_resolution_detection(audio_bytes=audio_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74899bed",
   "metadata": {},
   "source": [
    "### For Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XZocPYKVLtbq",
   "metadata": {},
   "source": [
    "#### Gemini Feedback Provider to evaluate Video Relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.feedback import Provider\n",
    "from pydantic import BaseModel, Field\n",
    "from google.genai import types\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class VideoRelevance(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the relevance classification of a recommended video\n",
    "    with respect to a given search query.\n",
    "    \"\"\"\n",
    "\n",
    "    value: Literal[\"relevant\", \"partially_relevant\", \"irrelevant\"] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The classification of the video's relevance to the search query. \"\n",
    "            \"'relevant' → directly addresses the main intent, \"\n",
    "            \"'partially_relevant' → overlaps but is incomplete or drifts, \"\n",
    "            \"'irrelevant' → does not address the query in a meaningful way.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    reason: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A concise explanation describing why this classification was chosen. \"\n",
    "            \"Should reference topic alignment, specificity, format/medium match, \"\n",
    "            \"and clarity of relevance.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class Gemini_Provider(Provider):\n",
    "    def video_relevance_scorer(self, query, video_bytes):\n",
    "        emotion = google_client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "You are an AI system designed to judge whether a recommended video is relevant to a given search query.\n",
    "\n",
    "### TASK:\n",
    "\n",
    "Analyze the provided search query and the recommended video (including its title, description, and thumbnail description if available).\n",
    "Determine whether the video’s main content is relevant to the search intent expressed in the query.\n",
    "\n",
    "Consider these factors:\n",
    "- **Topic Alignment**: Does the video content match the subject of the search query?\n",
    "- **Specificity**: Does it address the specific focus, details, or constraints of the query?\n",
    "- **Format & Medium**: If the query implies a certain type of content (tutorial, documentary, news, etc.), does the video match?\n",
    "- **Clarity of Relevance**: Is the connection to the query obvious or is it only loosely related?\n",
    "\n",
    "The classification must be one of the following:\n",
    "['relevant', 'partially_relevant', 'irrelevant']\n",
    "\n",
    "IMPORTANT:\n",
    "- \"relevant\" → Directly addresses the main intent of the query.\n",
    "- \"partially_relevant\" → Shares some overlap but is missing key details or drifts into unrelated topics.\n",
    "- \"irrelevant\" → Does not address the query’s intent in a meaningful way.\n",
    "- Avoid overusing \"partially_relevant\" — decide firmly whenever possible.\n",
    "\n",
    "************\n",
    "\n",
    "Here is the search query:\n",
    "\"\"\"\n",
    "                ),\n",
    "                types.Part(text=query),\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "\n",
    "Here is the recommended video information:\n",
    "\"\"\"\n",
    "                ),\n",
    "                types.Part(\n",
    "                    inline_data=types.Blob(data=video_bytes, mime_type=\"video/mp4\")\n",
    "                ),\n",
    "                types.Part(\n",
    "                    text=\"\"\"\n",
    "\n",
    "************\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Provide a single word from the list above representing the relevance classification.\n",
    "\n",
    "************\n",
    "\n",
    "EXAMPLE RESPONSE: relevant\n",
    "\n",
    "************\n",
    "\n",
    "Analyze the query and the recommended video and respond in this format.\"\"\"\n",
    "                ),\n",
    "            ],\n",
    "            config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": VideoRelevance,\n",
    "            },\n",
    "        )\n",
    "        return emotion.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-kYZL91SHfry",
   "metadata": {},
   "source": [
    "#### Test custom feedback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_provider = Gemini_Provider()\n",
    "\n",
    "# Only for videos of size <20Mb\n",
    "video_file_name = \"chameleon.mp4\"\n",
    "with open(video_file_name, 'rb') as f:\n",
    "    video_bytes = f.read()\n",
    "\n",
    "relevance = gemini_provider.video_relevance_scorer(query=\"Chameleon hunting it's prey\",video_bytes=video_bytes)\n",
    "relevance"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
